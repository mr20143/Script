{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1aQbrSmkyMKbHjOxTivkfP4eEX_3vSfwI",
      "authorship_tag": "ABX9TyOYLZY29wrsCT/AZHEaKYrN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr20143/Script/blob/main/Custom_DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path to the training directory\n",
        "train_dir = '/content/drive/MyDrive/Ed Folder/Kaggle_2_class - mixed/chest_xray/train'\n",
        "\n",
        "# Get the list of class directories\n",
        "class_dirs = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
        "\n",
        "# Define the path to the output file\n",
        "output_file_path = '/content/drive/MyDrive/test_counts.txt'\n",
        "\n",
        "# Open the output file for writing\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    # Count the number of images for each class\n",
        "    class_counts = {}\n",
        "    for class_dir in class_dirs:\n",
        "        class_path = os.path.join(train_dir, class_dir)\n",
        "        num_images = len(os.listdir(class_path))\n",
        "        class_counts[class_dir] = num_images\n",
        "\n",
        "    # Write the counts for each class to the file\n",
        "    for class_name, count in class_counts.items():\n",
        "        output_file.write(f'Class: {class_name}, Number of Images: {count}\\n')\n",
        "\n",
        "# Print confirmation message\n",
        "print(f\"Class counts have been written to '{output_file_path}'.\")"
      ],
      "metadata": {
        "id": "d-5xqcwT13NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# Define a function to print to both console and file\n",
        "def print_and_write(text, file):\n",
        "    print(text)\n",
        "    file.write(text + '\\n')\n",
        "\n",
        "# Redirect standard output to the file\n",
        "log_file_path = '/content/drive/MyDrive/training_logs.txt'\n",
        "log_file = open(log_file_path, 'w')\n",
        "sys.stdout = log_file\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_data = datasets.ImageFolder('/content/drive/MyDrive/Ed Folder/Kaggle_2_class - mixed/chest_xray/train', transform=transform)\n",
        "val_data = datasets.ImageFolder('/content/drive/MyDrive/Ed Folder/Kaggle_2_class - mixed/chest_xray/val', transform=transform)\n",
        "\n",
        "# Define the percentage of data to be used for training (50%)\n",
        "train_size = int(1 * len(train_data))\n",
        "indices = list(range(len(train_data)))\n",
        "np.random.shuffle(indices)\n",
        "train_idx = indices[:train_size]\n",
        "\n",
        "# Create data loaders with SubsetRandomSampler\n",
        "train_loader = DataLoader(train_data, batch_size=16, sampler=SubsetRandomSampler(train_idx))\n",
        "val_loader = DataLoader(val_data, batch_size=16)\n",
        "\n",
        "# Load pretrained DenseNet model\n",
        "model = torchvision.models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the model for binary classification\n",
        "num_ftrs = model.classifier.in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        preds = (outputs > 0.5).float()\n",
        "        correct_predictions += (preds == labels.float().unsqueeze(1)).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_targets = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=f'Validation'):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.squeeze(), labels.float())\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            preds = (outputs > 0.5).float()\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_targets.extend(labels.cpu().numpy())\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = accuracy_score(val_targets, val_preds)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print_and_write(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Learning Rate: {scheduler.get_last_lr()[0]}', log_file)\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/trained_model.pth')\n",
        "\n",
        "# Close the file\n",
        "log_file.close()\n",
        "\n",
        "# Restore standard output\n",
        "sys.stdout = sys.__stdout__\n"
      ],
      "metadata": {
        "id": "R4ssQvfr15K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define transforms for the test dataset (make sure they match the ones used during training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the test dataset\n",
        "test_data = datasets.ImageFolder('/content/drive/MyDrive/Ed Folder/Kaggle_2_class - mixed/chest_xray/train', transform=transform)\n",
        "\n",
        "# Create a data loader for the test dataset\n",
        "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load the pre-trained DenseNet model\n",
        "model = torchvision.models.densenet121(pretrained=False)  # Make sure pretrained is set to False\n",
        "num_ftrs = model.classifier.in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Load the saved weights of your trained model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/trained_model.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define a criterion for evaluation (e.g., Binary Cross Entropy Loss)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Define lists to store predictions and ground truth labels\n",
        "predictions = []\n",
        "targets = []\n",
        "losses = []\n",
        "\n",
        "# Iterate over the test dataset and make predictions\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels.float())\n",
        "        preds = (outputs > 0.5).float()\n",
        "        predictions.extend(preds.cpu().numpy())\n",
        "        targets.extend(labels.cpu().numpy())\n",
        "        losses.append(loss.item())  # Append the loss value\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(targets, predictions)\n",
        "\n",
        "# Calculate the average loss\n",
        "average_loss = sum(losses) / len(losses)\n",
        "\n",
        "# Print test accuracy and loss to a log file\n",
        "log_file_path = '/content/drive/MyDrive/test_logs.txt'\n",
        "with open(log_file_path, 'w') as log_file:\n",
        "    log_file.write(f'Test Accuracy: {accuracy:.4f}\\n')\n",
        "    log_file.write(f'Average Loss: {average_loss:.4f}\\n')\n"
      ],
      "metadata": {
        "id": "PEMC2vqhKMU4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}